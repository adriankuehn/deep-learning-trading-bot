Python 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license()" for more information.
>>> 
= RESTART: C:\Users\adria\OneDrive\Desktop_neu_2\Projekt_Delta_One\Das verändert alles !!!!!!!!!!!!! FXCM DATAPROV\Autoencoder\Autoencoder für Bott - Fertig\Best Autoencoder - FINAL\Autoencoder Final - Bot.py
Using TensorFlow backend.
Path:  Daten FERTIG/28.20.txt
Path:  Daten FERTIG/29.20.txt
Path:  Daten FERTIG/30.20.txt
Path:  Daten FERTIG/31.20.txt
len(L_train_States)  //  len(L_train_labels):  552   //   552
Path:  Daten FERTIG/32.20.txt
len(L_test_States)  //  len(L_test_labels):  257   //   257
Durchs.: Targ_Proz:  0.0092
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 399)]             0         
_________________________________________________________________
dense (Dense)                (None, 350)               140000    
_________________________________________________________________
dense_1 (Dense)              (None, 300)               105300    
_________________________________________________________________
dense_2 (Dense)              (None, 250)               75250     
_________________________________________________________________
dense_3 (Dense)              (None, 200)               50200     
_________________________________________________________________
dense_4 (Dense)              (None, 150)               30150     
_________________________________________________________________
dense_5 (Dense)              (None, 100)               15100     
_________________________________________________________________
dense_6 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_7 (Dense)              (None, 100)               5100      
_________________________________________________________________
dense_8 (Dense)              (None, 150)               15150     
_________________________________________________________________
dense_9 (Dense)              (None, 200)               30200     
_________________________________________________________________
dense_10 (Dense)             (None, 250)               50250     
_________________________________________________________________
dense_11 (Dense)             (None, 300)               75300     
_________________________________________________________________
dense_12 (Dense)             (None, 350)               105350    
_________________________________________________________________
dense_13 (Dense)             (None, 399)               140049    
=================================================================
Total params: 842,449
Trainable params: 842,449
Non-trainable params: 0
_________________________________________________________________
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 399)]             0         
_________________________________________________________________
dense (Dense)                (None, 350)               140000    
_________________________________________________________________
dense_1 (Dense)              (None, 300)               105300    
_________________________________________________________________
dense_2 (Dense)              (None, 250)               75250     
_________________________________________________________________
dense_3 (Dense)              (None, 200)               50200     
_________________________________________________________________
dense_4 (Dense)              (None, 150)               30150     
_________________________________________________________________
dense_5 (Dense)              (None, 100)               15100     
_________________________________________________________________
dense_6 (Dense)              (None, 50)                5050      
=================================================================
Total params: 421,050
Trainable params: 421,050
Non-trainable params: 0
_________________________________________________________________
BEGINNNING 1:  [0.44133333, 1.0, 1.0, 1.0, 0.20683334, 1.0, 1.0, 0.324, 0.38266667, 0.588, 0.5, 0.61733333, 0.0015, 0.20683334, 0.23616666, 0.35333333, 0.85183334, 0.06016667, 0.06016667, 0.64666667, 0.70533333, 1.0, 0.5, 0.14816666, 0.38266667, 0.412, 0.03083334, 0.588, 0.38266667, 1.0, 0.44133333, 1.0, 0.70516667, 0.67583333, 0.76383333, 0.79316666, 0.79316666, 0.76383333, 0.55866666, 0.0895, 0.0895, 0.9985, 0.52933334, 0.2655, 1.0, 0.2655, 0.52933334, 0.88116667, 0.79316666, 0.06016667]

BEGINNNING 2:  [0.8695, 0.898, 0.64216666, 0.78416666, 0.58533333, 0.75583333, 1.0, 0.58533333, 1.0, 0.41466666, 1.0, 0.44316667, 1.0, 1.0, 1.0, 1.0, 0.6705, 0.7275, 0.58533333, 0.21566666, 1.0, 1.0, 0.13033333, 0.21566666, 0.61366667, 0.4715, 0.699, 0.898, 0.5, 0.4715, 0.4715, 0.67066666, 1.0, 0.24416667, 0.61366667, 0.4715, 0.44316667, 1.0, 0.61366667, 1.0, 1.0, 0.4715, 1.0, 0.81266667, 0.44316667, 0.13033333, 1.0, 0.15883334, 0.4715, 0.41466666]

BEGINNNING 3:  [0.75366667, 0.30266667, 1.0, 1.0, 0.2745, 0.33083334, 0.18983333, 1.0, 0.049, 0.75366667, 0.97916666, 1.0, 1.0, 0.75366667, 0.38716666, 1.0, 0.95116667, 1.0, 0.5, 0.782, 0.16166666, 1.0, 0.4155, 0.07716667, 1.0, 0.951, 0.97933333, 0.951, 1.0, 1.0, 0.97916666, 0.52816666, 0.55633334, 0.61283334, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1335, 1.0, 1.0, 1.0, 1.0, 0.44366667, 0.30266667, 0.47183334, 0.5, 0.52816666, 0.16183334]

BEGINNNING 4:  [0.35816666, 0.38666667, 0.5, 1.0, 0.415, 0.2165, 0.10316666, 0.55666666, 0.84016666, 0.61333333, 0.55666666, 0.1315, 0.07483334, 0.35833333, 0.75516666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10316666, 1.0, 0.35833333, 1.0, 1.0, 0.44333333, 0.415, 0.24483333, 0.72683334, 1.0, 0.15983334, 1.0, 0.69833334, 0.33, 0.415, 0.1315, 0.61333333, 0.72666666, 0.01816667, 1.0, 0.5, 1.0, 0.75516666, 1.0, 1.0, 0.44333333, 0.30166667, 0.2165, 0.0465, 0.27333334]

BEGINNNING 5:  [1.0, 1.0, 0.89416666, 0.24666667, 0.697, 0.66883333, 0.64083333, 1.0, 0.9505, 0.75333333, 0.19033333, 1.0, 1.0, 0.75333333, 1.0, 1.0, 0.16216666, 0.16216666, 0.07766667, 1.0, 0.35933333, 1.0, 0.07783334, 1.0, 1.0, 0.83783334, 0.72516666, 0.07783334, 0.5, 0.35933333, 0.47183334, 0.64066667, 1.0, 0.5845, 1.0, 0.19033333, 1.0, 0.5845, 0.47183334, 0.83783334, 1.0, 0.86583334, 0.5, 0.2185, 0.16233334, 0.3875, 1.0, 0.92216666, 0.72516666, 1.0]

BEGINNNING 6:  [1.0, 0.52816666, 0.52816666, 0.4435, 0.27433334, 0.4435, 0.5565, 0.61283334, 0.47183334, 0.6975, 0.81033333, 0.5, 1.0, 0.895, 0.21783334, 0.58466666, 0.5565, 0.66933334, 1.0, 1.0, 0.97966666, 0.61283334, 0.33066666, 1.0, 1.0, 1.0, 0.52816666, 0.61283334, 0.6975, 0.1615, 0.24616667, 1.0, 1.0, 0.0205, 1.0, 0.6975, 0.24616667, 0.5, 0.641, 0.5565, 0.47183334, 1.0, 1.0, 1.0, 0.5, 0.33083334, 0.95133333, 1.0, 0.66933334, 1.0]

Epoch 1/100
552/552 - 3s - loss: 0.6833 - accuracy: 0.0000e+00 - val_loss: 0.6694 - val_accuracy: 0.0000e+00
Epoch 2/100
552/552 - 2s - loss: 0.6641 - accuracy: 0.0000e+00 - val_loss: 0.6485 - val_accuracy: 0.0000e+00
Epoch 3/100
552/552 - 2s - loss: 0.6491 - accuracy: 0.0000e+00 - val_loss: 0.6329 - val_accuracy: 0.0000e+00
Epoch 4/100
552/552 - 2s - loss: 0.6392 - accuracy: 0.0000e+00 - val_loss: 0.6226 - val_accuracy: 0.0000e+00
Epoch 5/100
552/552 - 3s - loss: 0.6334 - accuracy: 0.0000e+00 - val_loss: 0.6167 - val_accuracy: 0.0000e+00
Epoch 6/100
552/552 - 3s - loss: 0.6303 - accuracy: 0.0000e+00 - val_loss: 0.6133 - val_accuracy: 0.0000e+00
Epoch 7/100
552/552 - 2s - loss: 0.6288 - accuracy: 0.0036 - val_loss: 0.6114 - val_accuracy: 0.0000e+00
Epoch 8/100
552/552 - 2s - loss: 0.6280 - accuracy: 0.1649 - val_loss: 0.6103 - val_accuracy: 0.4553
Epoch 9/100
552/552 - 2s - loss: 0.6276 - accuracy: 0.4402 - val_loss: 0.6097 - val_accuracy: 0.5292
Epoch 10/100
552/552 - 2s - loss: 0.6274 - accuracy: 0.4982 - val_loss: 0.6094 - val_accuracy: 0.5331
Epoch 11/100
552/552 - 2s - loss: 0.6272 - accuracy: 0.4982 - val_loss: 0.6092 - val_accuracy: 0.5331
Epoch 12/100
552/552 - 2s - loss: 0.6272 - accuracy: 0.4982 - val_loss: 0.6091 - val_accuracy: 0.5331
Epoch 13/100
552/552 - 2s - loss: 0.6271 - accuracy: 0.4982 - val_loss: 0.6090 - val_accuracy: 0.5331
Epoch 14/100
552/552 - 2s - loss: 0.6271 - accuracy: 0.4982 - val_loss: 0.6090 - val_accuracy: 0.5331
Epoch 15/100
552/552 - 2s - loss: 0.6271 - accuracy: 0.4982 - val_loss: 0.6090 - val_accuracy: 0.5331
Epoch 16/100
552/552 - 2s - loss: 0.6271 - accuracy: 0.4982 - val_loss: 0.6090 - val_accuracy: 0.5331
Epoch 17/100
552/552 - 2s - loss: 0.6271 - accuracy: 0.4982 - val_loss: 0.6089 - val_accuracy: 0.5331
Epoch 18/100
552/552 - 2s - loss: 0.6271 - accuracy: 0.4982 - val_loss: 0.6089 - val_accuracy: 0.5331
Epoch 19/100
552/552 - 2s - loss: 0.6270 - accuracy: 0.4982 - val_loss: 0.6088 - val_accuracy: 0.5331
Epoch 20/100
552/552 - 2s - loss: 0.6270 - accuracy: 0.4982 - val_loss: 0.6089 - val_accuracy: 0.5331
Epoch 21/100
552/552 - 2s - loss: 0.6270 - accuracy: 0.4982 - val_loss: 0.6089 - val_accuracy: 0.5331
Epoch 22/100
552/552 - 2s - loss: 0.6270 - accuracy: 0.4982 - val_loss: 0.6090 - val_accuracy: 0.5331
Epoch 23/100
552/552 - 2s - loss: 0.6269 - accuracy: 0.4982 - val_loss: 0.6089 - val_accuracy: 0.5331
Epoch 24/100
552/552 - 2s - loss: 0.6269 - accuracy: 0.4982 - val_loss: 0.6089 - val_accuracy: 0.5331
Epoch 25/100
552/552 - 2s - loss: 0.6269 - accuracy: 0.4946 - val_loss: 0.6090 - val_accuracy: 0.5331
Epoch 26/100
552/552 - 3s - loss: 0.6268 - accuracy: 0.4928 - val_loss: 0.6088 - val_accuracy: 0.5331
Epoch 27/100
552/552 - 3s - loss: 0.6267 - accuracy: 0.4946 - val_loss: 0.6089 - val_accuracy: 0.5292
Epoch 28/100
552/552 - 2s - loss: 0.6266 - accuracy: 0.4837 - val_loss: 0.6088 - val_accuracy: 0.5292
Epoch 29/100
552/552 - 2s - loss: 0.6264 - accuracy: 0.4819 - val_loss: 0.6089 - val_accuracy: 0.5253
Epoch 30/100
552/552 - 2s - loss: 0.6262 - accuracy: 0.4728 - val_loss: 0.6084 - val_accuracy: 0.5253
Epoch 31/100
552/552 - 2s - loss: 0.6258 - accuracy: 0.4565 - val_loss: 0.6098 - val_accuracy: 0.3969
Epoch 32/100
552/552 - 2s - loss: 0.6253 - accuracy: 0.4420 - val_loss: 0.6087 - val_accuracy: 0.4436
Epoch 33/100
552/552 - 2s - loss: 0.6245 - accuracy: 0.4312 - val_loss: 0.6078 - val_accuracy: 0.4358
Epoch 34/100
552/552 - 2s - loss: 0.6234 - accuracy: 0.3732 - val_loss: 0.6174 - val_accuracy: 0.2140
Epoch 35/100
552/552 - 2s - loss: 0.6223 - accuracy: 0.3877 - val_loss: 0.6059 - val_accuracy: 0.4630
Epoch 36/100
552/552 - 2s - loss: 0.6209 - accuracy: 0.3623 - val_loss: 0.6052 - val_accuracy: 0.4825
Epoch 37/100
552/552 - 3s - loss: 0.6198 - accuracy: 0.3786 - val_loss: 0.6048 - val_accuracy: 0.4436
Epoch 38/100
552/552 - 3s - loss: 0.6190 - accuracy: 0.3822 - val_loss: 0.6044 - val_accuracy: 0.4436
Epoch 39/100
552/552 - 2s - loss: 0.6183 - accuracy: 0.3587 - val_loss: 0.6059 - val_accuracy: 0.3580
Epoch 40/100
552/552 - 2s - loss: 0.6177 - accuracy: 0.3841 - val_loss: 0.6044 - val_accuracy: 0.4708
Epoch 41/100
552/552 - 2s - loss: 0.6173 - accuracy: 0.3913 - val_loss: 0.6035 - val_accuracy: 0.4397
Epoch 42/100
552/552 - 2s - loss: 0.6170 - accuracy: 0.3768 - val_loss: 0.6033 - val_accuracy: 0.4475
Epoch 43/100
552/552 - 2s - loss: 0.6165 - accuracy: 0.3859 - val_loss: 0.6035 - val_accuracy: 0.4280
Epoch 44/100
552/552 - 2s - loss: 0.6162 - accuracy: 0.3841 - val_loss: 0.6029 - val_accuracy: 0.4786
Epoch 45/100
552/552 - 2s - loss: 0.6160 - accuracy: 0.3841 - val_loss: 0.6027 - val_accuracy: 0.4669
Epoch 46/100
552/552 - 2s - loss: 0.6157 - accuracy: 0.3696 - val_loss: 0.6026 - val_accuracy: 0.4358
Epoch 47/100
552/552 - 2s - loss: 0.6155 - accuracy: 0.3750 - val_loss: 0.6024 - val_accuracy: 0.4864
Epoch 48/100
552/552 - 2s - loss: 0.6153 - accuracy: 0.3841 - val_loss: 0.6023 - val_accuracy: 0.4708
Epoch 49/100
552/552 - 2s - loss: 0.6151 - accuracy: 0.3714 - val_loss: 0.6021 - val_accuracy: 0.4747
Epoch 50/100
552/552 - 3s - loss: 0.6149 - accuracy: 0.3732 - val_loss: 0.6022 - val_accuracy: 0.4475
Epoch 51/100
552/552 - 3s - loss: 0.6148 - accuracy: 0.3804 - val_loss: 0.6020 - val_accuracy: 0.4591
Epoch 52/100
552/552 - 2s - loss: 0.6146 - accuracy: 0.3732 - val_loss: 0.6019 - val_accuracy: 0.4591
Epoch 53/100
552/552 - 2s - loss: 0.6145 - accuracy: 0.3859 - val_loss: 0.6019 - val_accuracy: 0.4475
Epoch 54/100
552/552 - 2s - loss: 0.6144 - accuracy: 0.3696 - val_loss: 0.6017 - val_accuracy: 0.4553
Epoch 55/100
552/552 - 2s - loss: 0.6143 - accuracy: 0.3605 - val_loss: 0.6019 - val_accuracy: 0.4125
Epoch 56/100
552/552 - 2s - loss: 0.6141 - accuracy: 0.3533 - val_loss: 0.6016 - val_accuracy: 0.4630
Epoch 57/100
552/552 - 2s - loss: 0.6140 - accuracy: 0.3804 - val_loss: 0.6016 - val_accuracy: 0.4436
Epoch 58/100
552/552 - 2s - loss: 0.6140 - accuracy: 0.3587 - val_loss: 0.6014 - val_accuracy: 0.4786
Epoch 59/100
552/552 - 2s - loss: 0.6139 - accuracy: 0.3678 - val_loss: 0.6016 - val_accuracy: 0.4280
Epoch 60/100
552/552 - 2s - loss: 0.6137 - accuracy: 0.3569 - val_loss: 0.6014 - val_accuracy: 0.4786
Epoch 61/100
552/552 - 2s - loss: 0.6137 - accuracy: 0.3768 - val_loss: 0.6013 - val_accuracy: 0.4436
Epoch 62/100
552/552 - 2s - loss: 0.6136 - accuracy: 0.3551 - val_loss: 0.6013 - val_accuracy: 0.4319
Epoch 63/100
552/552 - 2s - loss: 0.6135 - accuracy: 0.3551 - val_loss: 0.6016 - val_accuracy: 0.3696
Epoch 64/100
552/552 - 2s - loss: 0.6134 - accuracy: 0.3478 - val_loss: 0.6012 - val_accuracy: 0.4475
Epoch 65/100
552/552 - 2s - loss: 0.6133 - accuracy: 0.3442 - val_loss: 0.6011 - val_accuracy: 0.3891
Epoch 66/100
552/552 - 2s - loss: 0.6132 - accuracy: 0.3551 - val_loss: 0.6012 - val_accuracy: 0.4202
Epoch 67/100
552/552 - 2s - loss: 0.6131 - accuracy: 0.3478 - val_loss: 0.6011 - val_accuracy: 0.4125
Epoch 68/100
552/552 - 2s - loss: 0.6131 - accuracy: 0.3569 - val_loss: 0.6010 - val_accuracy: 0.4241
Epoch 69/100
552/552 - 2s - loss: 0.6129 - accuracy: 0.3496 - val_loss: 0.6009 - val_accuracy: 0.4163
Epoch 70/100
552/552 - 2s - loss: 0.6128 - accuracy: 0.3188 - val_loss: 0.6009 - val_accuracy: 0.4202
Epoch 71/100
552/552 - 3s - loss: 0.6128 - accuracy: 0.3424 - val_loss: 0.6009 - val_accuracy: 0.4280
Epoch 72/100
552/552 - 2s - loss: 0.6127 - accuracy: 0.3406 - val_loss: 0.6008 - val_accuracy: 0.4008
Epoch 73/100
552/552 - 2s - loss: 0.6126 - accuracy: 0.3207 - val_loss: 0.6008 - val_accuracy: 0.3891
Epoch 74/100
552/552 - 3s - loss: 0.6125 - accuracy: 0.3279 - val_loss: 0.6008 - val_accuracy: 0.4125
Epoch 75/100
552/552 - 2s - loss: 0.6124 - accuracy: 0.3388 - val_loss: 0.6007 - val_accuracy: 0.3502
Epoch 76/100
552/552 - 2s - loss: 0.6123 - accuracy: 0.3170 - val_loss: 0.6006 - val_accuracy: 0.3580
Epoch 77/100
552/552 - 3s - loss: 0.6122 - accuracy: 0.3043 - val_loss: 0.6006 - val_accuracy: 0.3891
Epoch 78/100
552/552 - 3s - loss: 0.6121 - accuracy: 0.3170 - val_loss: 0.6010 - val_accuracy: 0.4280
Epoch 79/100
552/552 - 2s - loss: 0.6120 - accuracy: 0.3279 - val_loss: 0.6006 - val_accuracy: 0.3541
Epoch 80/100
552/552 - 2s - loss: 0.6119 - accuracy: 0.2899 - val_loss: 0.6005 - val_accuracy: 0.3580
Epoch 81/100
552/552 - 2s - loss: 0.6118 - accuracy: 0.3025 - val_loss: 0.6005 - val_accuracy: 0.3424
Epoch 82/100
552/552 - 2s - loss: 0.6117 - accuracy: 0.3080 - val_loss: 0.6004 - val_accuracy: 0.3463
Epoch 83/100
552/552 - 2s - loss: 0.6116 - accuracy: 0.3007 - val_loss: 0.6004 - val_accuracy: 0.3268
Epoch 84/100
552/552 - 2s - loss: 0.6115 - accuracy: 0.2808 - val_loss: 0.6003 - val_accuracy: 0.3502
Epoch 85/100
552/552 - 2s - loss: 0.6114 - accuracy: 0.3098 - val_loss: 0.6003 - val_accuracy: 0.3152
Epoch 86/100
552/552 - 2s - loss: 0.6113 - accuracy: 0.2844 - val_loss: 0.6003 - val_accuracy: 0.2918
Epoch 87/100
552/552 - 2s - loss: 0.6112 - accuracy: 0.2536 - val_loss: 0.6003 - val_accuracy: 0.3268
Epoch 88/100
552/552 - 2s - loss: 0.6111 - accuracy: 0.2627 - val_loss: 0.6001 - val_accuracy: 0.3502
Epoch 89/100
552/552 - 2s - loss: 0.6110 - accuracy: 0.2518 - val_loss: 0.6002 - val_accuracy: 0.3191
Epoch 90/100
552/552 - 2s - loss: 0.6109 - accuracy: 0.2591 - val_loss: 0.6001 - val_accuracy: 0.3230
Epoch 91/100
552/552 - 2s - loss: 0.6108 - accuracy: 0.2572 - val_loss: 0.6001 - val_accuracy: 0.3385
Epoch 92/100
552/552 - 2s - loss: 0.6107 - accuracy: 0.2500 - val_loss: 0.6001 - val_accuracy: 0.2957
Epoch 93/100
552/552 - 2s - loss: 0.6106 - accuracy: 0.2319 - val_loss: 0.6000 - val_accuracy: 0.3424
Epoch 94/100
552/552 - 2s - loss: 0.6104 - accuracy: 0.2319 - val_loss: 0.6000 - val_accuracy: 0.3035
Epoch 95/100
552/552 - 2s - loss: 0.6104 - accuracy: 0.2301 - val_loss: 0.6000 - val_accuracy: 0.3152
Epoch 96/100
552/552 - 2s - loss: 0.6102 - accuracy: 0.2391 - val_loss: 0.5999 - val_accuracy: 0.3191
Epoch 97/100
552/552 - 2s - loss: 0.6102 - accuracy: 0.2446 - val_loss: 0.5998 - val_accuracy: 0.3191
Epoch 98/100
552/552 - 2s - loss: 0.6100 - accuracy: 0.2174 - val_loss: 0.5998 - val_accuracy: 0.2685
Epoch 99/100
552/552 - 2s - loss: 0.6099 - accuracy: 0.2138 - val_loss: 0.5999 - val_accuracy: 0.2957
Epoch 100/100
552/552 - 2s - loss: 0.6098 - accuracy: 0.2138 - val_loss: 0.5998 - val_accuracy: 0.2879
ENDING 1:  [[ 0.50033545 -0.40639246  0.02965105  0.02834312  0.24098891 -0.3744625
  -0.06203391 -0.37938243  0.01608526  0.39596516  0.12068644 -0.17787266
   0.26206607  0.5329928   0.194021   -0.1819191  -0.33509395 -0.107536
   0.2777047   0.01067775  0.11732517  0.22293772 -0.07671713  0.06280711
  -0.05999865  0.00588986 -0.09583037  0.48227295 -0.0558158  -0.1960449
   0.04957406  0.3186863  -0.01976223  0.18811055 -0.31612527  0.02206833
   0.26596448  0.22820884 -0.29441252  0.01001749 -0.32441038 -0.10048985
   0.28937477  0.32832903 -0.5561093   0.18823472  0.3531372  -0.03721716
  -0.10680396 -0.04319467]]

ENDING 2:  [[ 0.18379974 -0.30220768  0.08156864 -0.03652172  0.17062075 -0.12560067
   0.28704834 -0.48193744 -0.0751825  -0.10750379  0.22409672 -0.10447714
   0.30274954  0.35135677  0.11466292 -0.3111583  -0.12514137  0.13267206
   0.21437272 -0.11867126  0.31971347  0.31056625  0.01342563 -0.1433133
  -0.19172992 -0.15458494 -0.21858276  0.33441982 -0.11589628 -0.3920557
   0.20777835  0.19774932  0.18417256  0.25797898 -0.4988507  -0.26370436
  -0.00861519  0.03990191 -0.25849316  0.15976942 -0.11871406 -0.0696942
  -0.38783452  0.24937029 -0.48820838  0.06929451 -0.1650737  -0.27755746
  -0.0464119   0.25089133]]

ENDING 3:  [[ 0.00188857 -0.2366366   0.47903824  0.37605253  0.13164586 -0.12946698
   0.31668752  0.254742   -0.06479058  0.41111034  0.09161618  0.08167548
  -0.34504923 -0.25319812 -0.22085568  0.07159457  0.03422594 -0.01258914
   0.23747379  0.1165266  -0.01311657  0.14216197 -0.3152071   0.391807
  -0.29198143  0.34080106  0.19855186 -0.0911003  -0.4755487   0.5805471
   0.3699365  -0.09517929  0.18468788 -0.1045765  -0.01031956 -0.13407244
  -0.21684831 -0.31047574 -0.00954473 -0.02327519  0.25927246 -0.09746379
   0.25932214  0.5745419  -0.1678145  -0.23816963 -0.05199957 -0.15027831
   0.05091539 -0.01367886]]

ENDING 4:  [[ 0.06428449 -0.27271605  0.20465897  0.03868372 -0.13544866  0.38718557
  -0.07889923 -0.03755356 -0.14528036  0.06500883 -0.2570329   0.2687912
  -0.2633936  -0.07456376 -0.22689083  0.07833145  0.13550916  0.46511337
   0.0634608  -0.46367097  0.22307986  0.00671867 -0.01854207  0.3911477
  -0.3168376   0.5327774  -0.02755764  0.06880306  0.208135    0.33540967
   0.5568689  -0.10631215 -0.496348    0.02568396 -0.04910998 -0.26377466
  -0.1569919   0.23357199  0.17093073 -0.11317381  0.32993686 -0.17485568
  -0.02411983  0.17308283 -0.6777912  -0.3396817  -0.42868507 -0.06851137
  -0.3382718   0.12759355]]

ENDING 5:  [[ 0.43035504 -0.6094673  -0.21201839 -0.31775236 -0.00834653  0.1594275
  -0.0205759  -0.398628    0.25927064 -0.18418209  0.18762796 -0.16128902
   0.07212011 -0.0798142  -0.02654363  0.18587613 -0.19250505 -0.04242972
  -0.36795515  0.23414493  0.23859514  0.02494012 -0.07793366  0.19307618
  -0.11383458  0.14272732 -0.1783785   0.63432926  0.46629065  0.29451892
   0.14588432 -0.5780495  -0.16656674  0.01293964 -0.16994382 -0.02910338
   0.13758756 -0.07849236 -0.30849582  0.5552594  -0.16481605 -0.25965932
  -0.27787393  0.10742807 -0.23951143 -0.24963672  0.0101449  -0.0455086
   0.08994813  0.3658114 ]]

ENDING 6:  [[ 0.326305    0.04941921  0.45553467  0.32863402 -0.05369021 -0.3903054
  -0.04875006 -0.281088    0.3918798   0.12468261  0.16009173  0.06558019
  -0.20513895  0.31547496  0.25768134 -0.36736834  0.00598118 -0.27734122
  -0.16365327  0.24459362  0.10879536 -0.2610486  -0.1585766   0.04218587
  -0.05544187 -0.3803954   0.47166473  0.44922736  0.10049247 -0.31422806
   0.32701916  0.10488755 -0.2800905   0.31684175 -0.38309845 -0.29551744
  -0.5428411  -0.21074992 -0.17488827  0.36661848 -0.07940581  0.12143959
  -0.34182957 -0.5893066   0.20337689  0.03548224  0.06599693 -0.37506363
   0.06290775 -0.2785805 ]]

ENDING AUTO 1:  [0.704352   0.7094359  0.68854606 0.73367274 0.7626034  0.7211005
 0.70806324 0.73381025 0.72199285 0.6755713  0.6668265  0.68823594
 0.6926312  0.67276204 0.6530123  0.6785792  0.6683541  0.6876578
 0.6740284  0.71516603 0.7202344  0.73740643 0.6928035  0.6896453
 0.6394967  0.68860036 0.72197    0.73641706 0.7292884  0.730206
 0.7636999  0.7515344  0.7726914  0.72551924 0.7837542  0.72573495
 0.73830044 0.74149835 0.66732335 0.70128936 0.7482743  0.6996945
 0.69549084 0.66668344 0.682673   0.6984652  0.7203526  0.70632553
 0.664912   0.6960838 ]

ENDING AUTO 2:  [0.6331759  0.6278346  0.6037161  0.6459471  0.6779061  0.6234183
 0.6587804  0.69909346 0.6096027  0.61489004 0.6233933  0.61455727
 0.62977517 0.6407106  0.6102372  0.6078784  0.59045637 0.6040828
 0.61794955 0.6617103  0.65678    0.6486391  0.57795084 0.6326081
 0.63059783 0.6164432  0.63312393 0.6292039  0.6208748  0.64104795
 0.65195036 0.66278976 0.7095066  0.61877286 0.68471265 0.6631106
 0.6675171  0.68769586 0.6063725  0.63360274 0.686211   0.6500214
 0.5974621  0.65030354 0.6527417  0.65795493 0.632438   0.6189248
 0.62340784 0.63049436]

ENDING AUTO 3:  [0.8064941  0.7997792  0.809291   0.80941194 0.7894077  0.79646325
 0.7778815  0.78606427 0.74752194 0.7441347  0.75053024 0.76653826
 0.7941116  0.76766604 0.7618722  0.79976046 0.79812795 0.8012326
 0.79920554 0.77444    0.80085325 0.80134344 0.80857706 0.80874085
 0.8361491  0.81799436 0.8177237  0.7974504  0.79700065 0.8133689
 0.7848879  0.80107677 0.8215023  0.79656893 0.8029628  0.7993351
 0.7829915  0.75734246 0.7839751  0.8036915  0.8306016  0.7932513
 0.7922677  0.7858436  0.7871     0.79238987 0.78497964 0.7788696
 0.79956365 0.785601  ]

ENDING AUTO 4:  [0.6828332  0.6778661  0.734707   0.6884334  0.6798386  0.6956979
 0.620566   0.60824156 0.602235   0.6474697  0.6507531  0.65314317
 0.6900459  0.6668953  0.6718047  0.7234938  0.7534158  0.6876731
 0.69341284 0.69038415 0.68112254 0.6587974  0.6959095  0.72376394
 0.7958846  0.6992041  0.75179744 0.69980985 0.6893904  0.71864825
 0.6340043  0.7009685  0.6602532  0.6063938  0.615212   0.6293343
 0.61823326 0.65521306 0.68110347 0.70079476 0.6796897  0.72657716
 0.7050413  0.6799441  0.6920407  0.6440775  0.6326699  0.62097937
 0.681762   0.65093106]

ENDING AUTO 5:  [0.68449175 0.61049145 0.6773829  0.67894495 0.65466803 0.65991473
 0.67063546 0.6652203  0.68661726 0.6658915  0.64785707 0.65275824
 0.6374861  0.6584425  0.67394555 0.64011276 0.6677327  0.7239306
 0.6930727  0.6940717  0.6953177  0.6780081  0.67436635 0.6987661
 0.6745496  0.69497585 0.65674865 0.68745184 0.71746993 0.67801005
 0.6835855  0.65940356 0.70139605 0.6698296  0.6964362  0.65363747
 0.7122916  0.69235814 0.67886233 0.6964862  0.6675917  0.6871483
 0.703851   0.65392834 0.6326004  0.6406579  0.6504369  0.6510582
 0.72259843 0.6727115 ]

ENDING AUTO 6:  [0.6900724  0.64144754 0.6436174  0.6437874  0.67111707 0.6514602
 0.5919429  0.66554195 0.62126666 0.6602434  0.7071085  0.67494667
 0.6586174  0.6330219  0.599688   0.66779566 0.6899613  0.6522378
 0.6755539  0.6607968  0.71943974 0.6430133  0.6899297  0.68061984
 0.7224089  0.68602985 0.7067275  0.6687574  0.6136422  0.67121994
 0.6335684  0.62241334 0.6604766  0.5842688  0.62807465 0.6656978
 0.6346493  0.6109061  0.71457714 0.6952786  0.744586   0.68747866
 0.58951825 0.65193045 0.6477559  0.6522553  0.70117146 0.66627663
 0.60240453 0.69343936]

